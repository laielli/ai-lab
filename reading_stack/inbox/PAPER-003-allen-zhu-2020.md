# Paper: Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning

- **ID**: PAPER-003
- **arXiv**: 2012.09816
- **Authors**: Zeyuan Allen-Zhu, Yuanzhi Li
- **Year**: 2020
- **Added**: 2026-01-13
- **Status**: summarized

## Why Read

This paper provides theoretical foundations for knowledge distillation and ensemble methods in deep learning, directly relevant to the lab's focus on mechanistic understanding of neural network learning and knowledge distillation theory. The multi-view data structure framework could inform theoretical understanding of feature learning dynamics.

## Focus Areas
- [x] Mechanistic DL Theory
- [ ] Feature Learning
- [x] Knowledge Distillation
- [ ] Theory-Inspired Applications

## Notes
Notes from a first-pass reading.

## Questions
- How does the multi-view data structure framework relate to feature learning in practice?
- What are the key differences between ensemble methods in deep learning vs traditional ML (boosting)?
- How does self-distillation implicitly combine ensemble and distillation?
- Can the theoretical insights inform practical distillation strategies?
- What assumptions are made about network architecture and training dynamics?
